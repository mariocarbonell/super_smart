{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Generacion de texto utilizando LLM para Google Colab\n",
        "\n",
        "Este notebook utiliza [https://github.com/oobabooga/text-generation-webui](https://github.com/oobabooga/text-generation-webui) para lanzar modelos conversacionales en modo chat.\n",
        "\n",
        "Lanza todas las celdas y al final del documento aparecera una URL de gradio que hace de tunel a la maquina local\n",
        "\n",
        "## Parametros\n",
        "\n",
        "* **save_logs_to_google_drive**: Guarda los logs en google drive para que persistan entre sesiones.\n",
        "* **text_streaming**: Permite que el texto aparezca en tiempo real en lugar de esperar a que se haya generado todo resultado.\n",
        "* **activate_sending_pictures**: Añade un menu para enviar imagenes al bot que automaticamente seran evaluadas utilizando BLIP.\n",
        "\n",
        "\n",
        "## Creditos\n",
        "\n",
        "Basado en [notebook original por 81300](https://colab.research.google.com/github/81300/AI-Notebooks/blob/main/Colab-TextGen-GPU.ipynb).\n"
      ],
      "metadata": {
        "id": "MFQl6-FjSYtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "PQ2utFb4Gm8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. Manten esta pestaña activa para prevenir que Colab te desconecte { display-mode: \"form\" }\n",
        "\n",
        "#@markdown Presiona el boton de play que aparece a continuacion:\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ],
      "metadata": {
        "id": "f7TVVj_z4flw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2. Instala la UI web\n",
        "#@markdown Ten en cuenta que estos modelos son pesados y la capa gratuita de GDrive solo cuenta con 15gb <br>\n",
        "save_logs_to_google_drive = True #@param {type:\"boolean\"} \n",
        "save_everything_to_google_drive = True #@param {type:\"boolean\"} \n",
        "install_gptq = True\n",
        "from IPython.display import clear_output\n",
        "if save_logs_to_google_drive:\n",
        "  import os\n",
        "  import shutil\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  base_folder = '/content/drive/MyDrive'\n",
        "\n",
        "if save_everything_to_google_drive:\n",
        "    import os\n",
        "    import shutil\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    base_folder = '/content/drive/MyDrive'\n",
        "    repo_dir = '/content/drive/MyDrive/text-generation-webui'\n",
        "    model_dir = '/content/drive/MyDrive/text-generation-webui/models'\n",
        "    gptq_dir = '/content/drive/MyDrive/text-generation-webui/repositories/GPTQ-for-LLaMa'\n",
        "    if os.path.exists(repo_dir):\n",
        "        %cd {repo_dir}\n",
        "        !git pull\n",
        "    else:\n",
        "        %cd /content/drive/MyDrive/\n",
        "        !git clone https://github.com/oobabooga/text-generation-webui\n",
        "\n",
        "else:\n",
        "    model_dir = '/content/text-generation-webui/models'\n",
        "    repo_dir = '/content/text-generation-webui'\n",
        "    %cd /content\n",
        "    !git clone https://github.com/oobabooga/text-generation-webui\n",
        "\n",
        "\n",
        "\n",
        "if save_logs_to_google_drive:\n",
        "  if not os.path.exists(f\"{base_folder}/oobabooga-data\"):\n",
        "    os.mkdir(f\"{base_folder}/oobabooga-data\")\n",
        "  if not os.path.exists(f\"{base_folder}/oobabooga-data/logs\"):\n",
        "    os.mkdir(f\"{base_folder}/oobabooga-data/logs\")\n",
        "  if not os.path.exists(f\"{base_folder}/oobabooga-data/softprompts\"):\n",
        "    os.mkdir(f\"{base_folder}/oobabooga-data/softprompts\")\n",
        "  if not os.path.exists(f\"{base_folder}/oobabooga-data/characters\"):\n",
        "    shutil.move(\"text-generation-webui/characters\", f\"{base_folder}/oobabooga-data/characters\")\n",
        "  else:\n",
        "    !rm -r \"text-generation-webui/characters\"\n",
        "    \n",
        "  !rm -r \"text-generation-webui/softprompts\"\n",
        "  !ln -s \"$base_folder/oobabooga-data/logs\" \"text-generation-webui/logs\"\n",
        "  !ln -s \"$base_folder/oobabooga-data/softprompts\" \"text-generation-webui/softprompts\"\n",
        "  !ln -s \"$base_folder/oobabooga-data/characters\" \"text-generation-webui/characters\"\n",
        "\n",
        "else:\n",
        "  !mkdir text-generation-webui/logs\n",
        "\n",
        "!ln -s text-generation-webui/logs .\n",
        "!ln -s text-generation-webui/characters .\n",
        "!ln -s text-generation-webui/models .\n",
        "%rm -r sample_data\n",
        "%cd text-generation-webui\n",
        "!wget https://raw.githubusercontent.com/pcrii/Philo-Colab-Collection/main/settings-colab-template.json -O settings-colab-template.json\n",
        "\n",
        "# if not os.path.exists(f\"{repo_dir}/autograd_4bit.py\"):\n",
        "#   !wget https://raw.githubusercontent.com/johnsmith0031/alpaca_lora_4bit/main/autograd_4bit.py -O autograd_4bit.py\n",
        "\n",
        "# if not os.path.exists(f\"{repo_dir}/amp_wrapper.py\"):\n",
        "#   !wget https://raw.githubusercontent.com/johnsmith0031/alpaca_lora_4bit/main/amp_wrapper.py -O amp_wrapper.py\n",
        "\n",
        "# Install requirements\n",
        "!pip install -r requirements.txt --upgrade\n",
        "!pip install -r extensions/google_translate/requirements.txt\n",
        "!pip install -r extensions/silero_tts/requirements.txt\n",
        "print(f\"\\033[1;32;1m\\n --> If you see a warning about \\\"pydevd_plugins\\\", just ignore it and move on to Step 3. There is no need to restart the runtime.\\n\\033[0;37;0m\")\n",
        "\n",
        "if install_gptq:\n",
        "    if save_everything_to_google_drive:\n",
        "        if os.path.exists(gptq_dir):\n",
        "            %cd {gptq_dir}\n",
        "            !git pull\n",
        "            !pip install ninja\n",
        "            !pip install -r requirements.txt\n",
        "            !python setup_cuda.py install\n",
        "\n",
        "        else:\n",
        "            !mkdir /content/drive/MyDrive/text-generation-webui/repositories\n",
        "            %cd /content/drive/MyDrive/text-generation-webui/repositories\n",
        "            !git clone https://github.com/oobabooga/GPTQ-for-LLaMa.git -b cuda\n",
        "            %cd GPTQ-for-LLaMa\n",
        "            !pip install ninja\n",
        "            !pip install -r requirements.txt\n",
        "            !python setup_cuda.py install\n",
        "    else:\n",
        "        %mkdir /content/text-generation-webui/repositories/\n",
        "        %cd /content/text-generation-webui/repositories/\n",
        "        !git clone https://github.com/oobabooga/GPTQ-for-LLaMa.git -b cuda\n",
        "        %cd GPTQ-for-LLaMa\n",
        "        !pip install ninja\n",
        "        !pip install -r requirements.txt\n",
        "        !python setup_cuda.py install\n",
        "\n",
        "\n",
        "if not os.path.exists(f\"{repo_dir}/repositories\"):\n",
        "  os.mkdir(f\"{repo_dir}/repositories\")\n",
        "\n",
        "\n",
        "if os.path.exists(f\"{repo_dir}/repositories/alpaca_lora_4bit\"):\n",
        "  %cd {repo_dir}/repositories/alpaca_lora_4bit\n",
        "  !git pull\n",
        "  !pip install -r requirements.txt\n",
        "else:\n",
        "  %cd {repo_dir}/repositories/\n",
        "  !git clone https://github.com/johnsmith0031/alpaca_lora_4bit\n",
        "  %cd alpaca_lora_4bit\n",
        "  !pip install -r requirements.txt\n",
        "\n",
        "!pip install git+https://github.com/sterlind/GPTQ-for-LLaMa.git@lora_4bit\n",
        "\n",
        "# clear_output()\n",
        "# print(\"Finished\")"
      ],
      "metadata": {
        "id": "LGQ8BiMuXMDG",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Descarga el modelo\n",
        "#@markdown Puedes insertar cualquier modelo de huggingface con el formato Organizacion/modelo\n",
        "model_download = \"anon8231489123/vicuna-13b-GPTQ-4bit-128g\" #@param [\"anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g\", \"anon8231489123/vicuna-13b-GPTQ-4bit-128g\"] {allow-input: true}\n",
        "%cd {repo_dir}\n",
        "!python download-model.py {model_download}\n",
        "!rm {model_dir}/place-your-models-here.txt\n",
        "clear_output()\n",
        "if save_logs_to_google_drive or save_everything_to_google_drive:\n",
        "    drive_NOT_mounted = False\n",
        "else:\n",
        "    drive_NOT_mounted = True\n",
        "\n",
        "if drive_NOT_mounted:\n",
        "  import os\n",
        "print(\"Available Models\")\n",
        "print(os.listdir(model_dir))\n",
        "\n"
      ],
      "metadata": {
        "id": "E7t4QfXf4U9p",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3. Inicia la interfaz\n",
        "\n",
        "import json\n",
        "\n",
        "model_load = \"anon8231489123_vicuna-13b-GPTQ-4bit-128g\" #@param {type:\"string\"}\n",
        "# Parameters\n",
        "load_4bit_models = True #@param {type:\"boolean\"}\n",
        "\n",
        "groupsize_128 = True \n",
        "load_in_8bit = False \n",
        "chat = False \n",
        "\n",
        "text_streaming = True #@param {type:\"boolean\"}\n",
        "activate_silero_text_to_speech = False\n",
        "activate_sending_pictures = False #@param {type:\"boolean\"}\n",
        "activate_character_bias = False\n",
        "chat_language = \"English\" \n",
        "\n",
        "\n",
        "activate_google_translate = (chat_language != \"English\")\n",
        "\n",
        "language_codes = {'Afrikaans': 'af', 'Albanian': 'sq', 'Amharic': 'am', 'Arabic': 'ar', 'Armenian': 'hy', 'Azerbaijani': 'az', 'Basque': 'eu', 'Belarusian': 'be', 'Bengali': 'bn', 'Bosnian': 'bs', 'Bulgarian': 'bg', 'Catalan': 'ca', 'Cebuano': 'ceb', 'Chinese (Simplified)': 'zh-CN', 'Chinese (Traditional)': 'zh-TW', 'Corsican': 'co', 'Croatian': 'hr', 'Czech': 'cs', 'Danish': 'da', 'Dutch': 'nl', 'English': 'en', 'Esperanto': 'eo', 'Estonian': 'et', 'Finnish': 'fi', 'French': 'fr', 'Frisian': 'fy', 'Galician': 'gl', 'Georgian': 'ka', 'German': 'de', 'Greek': 'el', 'Gujarati': 'gu', 'Haitian Creole': 'ht', 'Hausa': 'ha', 'Hawaiian': 'haw', 'Hebrew': 'iw', 'Hindi': 'hi', 'Hmong': 'hmn', 'Hungarian': 'hu', 'Icelandic': 'is', 'Igbo': 'ig', 'Indonesian': 'id', 'Irish': 'ga', 'Italian': 'it', 'Japanese': 'ja', 'Javanese': 'jw', 'Kannada': 'kn', 'Kazakh': 'kk', 'Khmer': 'km', 'Korean': 'ko', 'Kurdish': 'ku', 'Kyrgyz': 'ky', 'Lao': 'lo', 'Latin': 'la', 'Latvian': 'lv', 'Lithuanian': 'lt', 'Luxembourgish': 'lb', 'Macedonian': 'mk', 'Malagasy': 'mg', 'Malay': 'ms', 'Malayalam': 'ml', 'Maltese': 'mt', 'Maori': 'mi', 'Marathi': 'mr', 'Mongolian': 'mn', 'Myanmar (Burmese)': 'my', 'Nepali': 'ne', 'Norwegian': 'no', 'Nyanja (Chichewa)': 'ny', 'Pashto': 'ps', 'Persian': 'fa', 'Polish': 'pl', 'Portuguese (Portugal, Brazil)': 'pt', 'Punjabi': 'pa', 'Romanian': 'ro', 'Russian': 'ru', 'Samoan': 'sm', 'Scots Gaelic': 'gd', 'Serbian': 'sr', 'Sesotho': 'st', 'Shona': 'sn', 'Sindhi': 'sd', 'Sinhala (Sinhalese)': 'si', 'Slovak': 'sk', 'Slovenian': 'sl', 'Somali': 'so', 'Spanish': 'es', 'Sundanese': 'su', 'Swahili': 'sw', 'Swedish': 'sv', 'Tagalog (Filipino)': 'tl', 'Tajik': 'tg', 'Tamil': 'ta', 'Telugu': 'te', 'Thai': 'th', 'Turkish': 'tr', 'Ukrainian': 'uk', 'Urdu': 'ur', 'Uzbek': 'uz', 'Vietnamese': 'vi', 'Welsh': 'cy', 'Xhosa': 'xh', 'Yiddish': 'yi', 'Yoruba': 'yo', 'Zulu': 'zu'}\n",
        "\n",
        "\n",
        "%cd {repo_dir}\n",
        "# Applying the selected language and setting the prompt size to 2048\n",
        "# if 8bit mode is selected\n",
        "j = json.loads(open('settings-colab-template.json', 'r').read())\n",
        "j[\"google_translate-language string\"] = language_codes[chat_language]\n",
        "if load_in_8bit:\n",
        "  j[\"chat_prompt_size\"] = 2048\n",
        "with open('settings-colab.json', 'w') as f:\n",
        "  f.write(json.dumps(j, indent=4))\n",
        "\n",
        "params = set()\n",
        "if chat:\n",
        "  params.add('--cai-chat')\n",
        "\n",
        "if load_in_8bit:\n",
        "  params.add('--load-in-8bit')\n",
        "#if auto_devices:\n",
        "#  params.add('--auto-devices')\n",
        "if load_4bit_models:\n",
        "  params.add('--wbits 4')\n",
        "\n",
        "if groupsize_128:\n",
        "  params.add('--groupsize 128')\n",
        "\n",
        "active_extensions = []\n",
        "if activate_sending_pictures:\n",
        "  active_extensions.append('send_pictures')\n",
        "if activate_character_bias:\n",
        "  active_extensions.append('character_bias')\n",
        "if activate_google_translate:\n",
        "  active_extensions.append('google_translate')\n",
        "if activate_silero_text_to_speech:\n",
        "  active_extensions.append('silero_tts')\n",
        "\n",
        "if len(active_extensions) > 0:\n",
        "  params.add(f'--extensions {\" \".join(active_extensions)}')\n",
        "\n",
        "if not text_streaming or activate_google_translate or activate_silero_text_to_speech:\n",
        "  params.add('--no-stream')\n",
        "if activate_character_bias:\n",
        "  params.add('--verbose')\n",
        "\n",
        "params.add('--monkey-patch')\n",
        "\n",
        "# Starting the web UI\n",
        "cmd = f\"python server.py --share --model {model_load} --settings settings-colab.json {' '.join(params)}\"\n",
        "print(cmd)\n",
        "!$cmd"
      ],
      "metadata": {
        "id": "hKuocueuXnm5",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}